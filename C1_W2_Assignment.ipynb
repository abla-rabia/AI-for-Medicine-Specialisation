{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Diagnostic Models\n",
    "\n",
    "Welcome to the second assignment of course 1. In this assignment, we will be working with the results of the X-ray classification model we developed in the previous assignment. In order to make the data processing a bit more manageable, we will be working with a subset of our training, and validation datasets. We will also use our manually labeled test dataset of 420 X-rays.\n",
    "\n",
    "As a reminder, our dataset contains X-rays from 14 different conditions diagnosable from an X-ray. We'll evaluate our performance on each of these classes using the classification metrics we learned in lecture.\n",
    "\n",
    "**By the end of this assignment you will learn about:**\n",
    "\n",
    "1. Accuracy\n",
    "2. Prevalence\n",
    "3. Specificity & Sensitivity\n",
    "4. PPV and NPV\n",
    "5. ROC curve and AUCROC (c-statistic)\n",
    "6. Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1. Packages](#1)\n",
    "- [2. Overview](#2)\n",
    "- [3. Metrics](#3)\n",
    "    - [3.1 - True Positives, False Positives, True Negatives and False Negatives](#3-1)\n",
    "        - [Exercise 1 - true positives, false positives, true negatives, and false negatives](#ex-1)\n",
    "    - [3.2 - Accuracy](#3-2)\n",
    "        - [Exercise 2 - get_accuracy](#ex-2)\n",
    "    - [3.3 Prevalence](#3-3)\n",
    "        - [Exercise 3 - get_prevalence](#ex-3)\n",
    "    - [3.4 Sensitivity and Specificity](#3-4)\n",
    "        - [Exercise 4 - get_sensitivity and get_specificity](#ex-4)\n",
    "    - [3.5 PPV and NPV](#3-5)\n",
    "        - [Exercise 5 - get_ppv and get_npv](#ex-5)\n",
    "    - [3.6 ROC Curve](#3-6)\n",
    "- [4. Confidence Intervals](#4)\n",
    "- [5. Precision-Recall Curve](#5)\n",
    "- [6. F1 Score](#6)\n",
    "- [7. Calibration](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages <a name='1'></a>\n",
    "\n",
    "We'll use:\n",
    "- numpy: scientific computing\n",
    "- matplotlib: visualization\n",
    "- pandas: data manipulation\n",
    "- sklearn: performance metrics\n",
    "- util, public_tests, test_utils: provided utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd  \n",
    "\n",
    "import util\n",
    "from public_tests import *\n",
    "from test_utils import *"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview <a name='2'></a>\n",
    "\n",
    "We'll go through our evaluation metrics in the following order:\n",
    "- TP, TN, FP, FN\n",
    "- Accuracy\n",
    "- Prevalence\n",
    "- Sensitivity and Specificity\n",
    "- PPV and NPV\n",
    "- AUC\n",
    "- Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_results = pd.read_csv(\"data/train_preds.csv\")\n",
    "valid_results = pd.read_csv(\"data/valid_preds.csv\")\n",
    "\n",
    "class_labels = ['Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia', 'Infiltration',\n",
    " 'Mass', 'Nodule', 'Atelectasis', 'Pneumothorax', 'Pleural_Thickening',\n",
    " 'Pneumonia', 'Fibrosis', 'Edema', 'Consolidation']\n",
    "pred_labels = [l + \"_pred\" for l in class_labels]\n",
    "\n",
    "y = valid_results[class_labels].values\n",
    "pred = valid_results[pred_labels].values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# peek at the dataset\n",
    "valid_results[np.concatenate([class_labels, pred_labels])].head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x=class_labels, height=y.sum(axis=0));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metrics <a name='3'></a>\n",
    "\n",
    "### 3.1 True Positives, False Positives, True Negatives and False Negatives <a name='3-1'></a>\n",
    "#### Exercise 1 - true positives, false positives, true negatives, and false negatives <a name='ex-1'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def true_positives(y, pred, th=0.5):\n",
    "    TP = np.sum((y == 1) & (pred >= th))\n",
    "    return TP\n",
    "\n",
    "def true_negatives(y, pred, th=0.5):\n",
    "    TN = np.sum((y == 0) & (pred < th))\n",
    "    return TN\n",
    "\n",
    "def false_positives(y, pred, th=0.5):\n",
    "    FP = np.sum((y == 0) & (pred >= th))\n",
    "    return FP\n",
    "\n",
    "def false_negatives(y, pred, th=0.5):\n",
    "    FN = np.sum((y == 1) & (pred < th))\n",
    "    return FN"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# test\n",
    "get_tp_tn_fp_fn_test(true_positives, true_negatives, false_positives, false_negatives)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "util.get_performance_metrics(y, pred, class_labels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Accuracy <a name='3-2'></a>\n",
    "#### Exercise 2 - get_accuracy <a name='ex-2'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_accuracy(y, pred, th=0.5):\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return accuracy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_accuracy_test(get_accuracy)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# What if we predicted all zeros for 'Emphysema'?\n",
    "get_accuracy(valid_results[\"Emphysema\"].values, np.zeros(len(valid_results)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Prevalence <a name='3-3'></a>\n",
    "#### Exercise 3 - get_prevalence <a name='ex-3'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_prevalence(y):\n",
    "    prevalence = np.mean(y == 1)\n",
    "    return prevalence"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_prevalence_test(get_prevalence)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Sensitivity and Specificity <a name='3-4'></a>\n",
    "#### Exercise 4 - get_sensitivity and get_specificity <a name='ex-4'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_sensitivity(y, pred, th=0.5):\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    return sensitivity\n",
    "\n",
    "def get_specificity(y, pred, th=0.5):\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    return specificity"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_sensitivity_specificity_test(get_sensitivity, get_specificity)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 PPV and NPV <a name='3-5'></a>\n",
    "#### Exercise 5 - get_ppv and get_npv <a name='ex-5'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_ppv(y, pred, th=0.5):\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    PPV = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    return PPV\n",
    "\n",
    "def get_npv(y, pred, th=0.5):\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "    NPV = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "    return NPV"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_ppv_npv_test(get_ppv, get_npv)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 ROC Curve <a name='3-6'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "util.get_curve(y, pred, class_labels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confidence Intervals <a name='4'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def bootstrap_auc(y, pred, classes, bootstraps = 100, fold_size = 1000):\n",
    "    statistics = np.zeros((len(classes), bootstraps))\n",
    "\n",
    "    for c in range(len(classes)):\n",
    "        df = pd.DataFrame(columns=['y', 'pred'])\n",
    "        df.loc[:, 'y'] = y[:, c]\n",
    "        df.loc[:, 'pred'] = pred[:, c]\n",
    "        # get positive examples for stratified sampling\n",
    "        df_pos = df[df.y == 1]\n",
    "        df_neg = df[df.y == 0]\n",
    "        prevalence = len(df_pos) / len(df)\n",
    "        for i in range(bootstraps):\n",
    "            # stratified sampling of positive and negative examples\n",
    "            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n",
    "            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n",
    "\n",
    "            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n",
    "            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n",
    "            score = roc_auc_score(y_sample, pred_sample)\n",
    "            statistics[c][i] = score\n",
    "    return statistics\n",
    "\n",
    "statistics = bootstrap_auc(y, pred, class_labels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "util.print_confidence_intervals(class_labels, statistics)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Precision-Recall Curve <a name='5'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "util.get_curve(y, pred, class_labels, curve='prc')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. F1 Score <a name='6'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import f1_score\n",
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score, f1=f1_score)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calibration <a name='7'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "def plot_calibration_curve(y, pred):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(class_labels)):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y[:,i], pred[:,i], n_bins=20)\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "        plt.plot(mean_predicted_value, fraction_of_positives, marker='.')\n",
    "        plt.xlabel(\"Predicted Value\")\n",
    "        plt.ylabel(\"Fraction of Positives\")\n",
    "        plt.title(class_labels[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_calibration_curve(y, pred)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR \n",
    "\n",
    "y_train = train_results[class_labels].values\n",
    "pred_train = train_results[pred_labels].values\n",
    "pred_calibrated = np.zeros_like(pred)\n",
    "\n",
    "for i in range(len(class_labels)):\n",
    "    lr = LR(solver='liblinear', max_iter=10000)\n",
    "    lr.fit(pred_train[:, i].reshape(-1, 1), y_train[:, i])    \n",
    "    pred_calibrated[:, i] = lr.predict_proba(pred[:, i].reshape(-1, 1))[:,1]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_calibration_curve(y, pred_calibrated)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## That's it!\n",
    "Congratulations! That was a lot of metrics to get familiarized with.\n",
    "We hope that you feel a lot more confident in your understanding of medical diagnostic evaluation and test your models correctly in your future work :)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7"
  },
  "title": "Evaluation of Diagnostic Models"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}